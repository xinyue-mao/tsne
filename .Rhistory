knitr::opts_chunk$set(echo=TRUE, error=FALSE, warning=FALSE)
library(calculus)
library(rje)
time = c(0.25, 0.50, 0.75, 1.00, 1.25, 2.00, 3.00, 4.00, 5.00, 6.00, 8.00)
conc = c(2.05, 1.04, 0.81, 0.39, 0.30, 0.23, 0.13, 0.11, 0.08, 0.10, 0.06)
sub5 = as.data.frame(t(rbind(time, conc)))
exp.func = function(x,b1,b2,b3,b4) {
return(exp(b1)*exp(-exp(b2)*x)+exp(b3)*exp(-exp(b4)*x))
}
# get X matrix without using numDeriv, hessian, jacobian, grad, or similar
get_score2 = function (x, b){
score = matrix(0, nrow = length(x), ncol = length(b))
for (j in c(1:length(x))){
xj = x[j]
partial1 = exp(b[1])*exp(-exp(b[2])*xj)
partial2 = -exp(b[1])*exp(-exp(b[2])*xj)*exp(b[2])*xj
partial3 = exp(b[3])*exp(-exp(b[4])*xj)
partial4 = -exp(b[3])*exp(-exp(b[4])*xj)*exp(b[4])*xj
score[j,] = c(partial1, partial2, partial3, partial4)
}
return(score)
}
## initial values
beta0_vec = c(1,1/2,-3/2,-3/2)
iter = 0                 ### track on the number of iterations
imax = 200               ### maximum iterations
betak = beta0_vec
conv = NULL              ### convergence status
finish = F               ### while loop status
## iterations starts here
while (finish == F) {
iter = iter + 1
## calculate score matrix (X)
X = get_score2(time, betak)
## update beta
newbetak = betak + solve(t(X) %*% X) %*% t(X)%*%
(conc-exp.func(b1=betak[1],b2=betak[2],b3=betak[3],b4=betak[4],x=time))
## difference between beta from current and previous iterations
beta_dist = norm(newbetak - betak, type="2")
betak = newbetak
## calculate the objective function value Dk
Dk = sum((conc - exp.func(x=time, b1=betak[1],b2=betak[2]
,b3=betak[3],b4=betak[4]))^2)
## stopping criteria
if (iter > imax) {finish = T; conv = "diverge"}
else if (beta_dist < 0.00001) {finish = T; conv = "converge"}
}
betaF_ols = betak
t(betaF_ols)
## model-based standard error
residuals = conc-exp.func(x=time,b1=betaF_ols[1],
b2=betaF_ols[2],b3=betaF_ols[3],b4=betaF_ols[4])
sigma_sq_hat_ols = sum(residuals^2)/(length(time)-length(betaF_ols))
X = get_score2(time,betaF_ols)
var_beta_hat = sigma_sq_hat_ols*solve(t(X)%*%X)
sd_beta_hat_ols = diag(sqrt(var_beta_hat))
sd_beta_hat_ols
biexpobj.weighted <- function(b,x,y) {
mu <- exp(b[1]-exp(b[2])*x)+exp(b[3]-exp(b[4])*x)
(y-mu)/mu
}
fit_hetero_nls = nls( ~ biexpobj.weighted(b,time,conc), data = sub5,
start = list(b = c(1,1/2,-3/2,-3/2) ), trace = T)
beta_hetero_nls = coef(fit_hetero_nls)
sigma_hat_hetero_nls = 0.1698
mbsd_hetero_nls = c(0.2399,0.1673,0.2109,0.2261)
summary(fit_hetero_nls)
## initial values
beta0_vec = c(1,1/2,-3/2,-3/2)
#beta0_vec = c(-0.3253211,0.3817617,-1.75843,-1.956932)
iter = 0                 ### track on the number of iterations
imax = 200               ### maximum iterations
betak = beta0_vec
conv = NULL              ### convergence status
finish = F               ### while loop status
## iterations starts here
while (finish == F) {
iter = iter + 1
## calculate score matrix (X)
X = get_score2(time, betak)
## calculate weight matrix (W)
w = (exp.func(time, b1=betak[1], b2=betak[2], b3=betak[3], b4=betak[4]))^(-2)
W = diag(w, nrow=length(time), ncol=length(time))
## update beta
newbetak = betak + solve(t(X) %*% W %*% X) %*% t(X) %*% W %*%
(conc-exp.func(b1=betak[1],b2=betak[2],b3=betak[3],b4=betak[4],x=time))
## difference between beta from current and previous iterations
beta_dist = norm(newbetak - betak, type="2")
betak = newbetak
## calculate the objective function value Dk
Dk = sum((conc - exp.func(x=time, b1=betak[1],b2=betak[2]
,b3=betak[3],b4=betak[4]))^2)
cat("iteration",iter,"\n")
cat("distance between beta", beta_dist,"; objective function value", Dk, "\n")
cat("betak ", betak[1], betak[2],betak[3],betak[4],"\n")
## stopping criteria
if (iter > imax) {finish = T; conv = "diverge"}
else if (beta_dist < 0.00001) {finish = T; conv = "converge"}
}
betaF_hetero_w = betak
t(betaF_hetero_w)
sigma_sq_hat_homemade_nls = 1/(length(time)-length(betak)) * sum(w %*% (conc -
exp.func( b1=betaF_hetero_w[1], b2=betaF_hetero_w[2],
b3=betaF_hetero_w[3], b4=betaF_hetero_w[4], x=time))^2)
sqrt(sigma_sq_hat_homemade_nls)
## model-based standard errors
mb_cov_mat_homemade_nls = solve(t(X) %*% W %*% X)*(sigma_sq_hat_homemade_nls)
mbsd_homemade_nls = sqrt(diag(mb_cov_mat_homemade_nls))
mbsd_homemade_nls
homemade_ols = c(betaF_ols, sd_beta_hat_ols, sqrt(sigma_sq_hat_ols))
func_hetero_nls = c(beta_hetero_nls, mbsd_hetero_nls, sigma_hat_hetero_nls)
homemade_hetero_nls = c(betaF_hetero_w, mbsd_homemade_nls,
sqrt(sigma_sq_hat_homemade_nls))
table1 = rbind(homemade_ols,func_hetero_nls,homemade_hetero_nls)
rownames(table1) = c("homemade OLS","R func NLS", "homemade WLS")
colnames(table1) = c("b1","b2","b3","b4","se(b1)","se(b2)","se(b3)","se(b4)","sigma")
knitr::kable(table1, digits = 3)
## initial values
beta0_vec = c(-1,1,1)
n = dim(infert)[1]
X = as.matrix(cbind(one=rep(1,n),infert[,c(6,4)]))
Y = infert[5]
iter = 0                 ### track on the number of iterations
imax = 500               ### maximum iterations
betak = beta0_vec
conv = NULL              ### convergence status
finish = F               ### while loop status
## iterations starts here
while (finish == F) {
iter = iter + 1
## calculate weight matrix (W)
Xbeta = X %*% betak
w = as.numeric((expit(Xbeta)*(1-expit(Xbeta))))
W = diag(w, nrow=n, ncol=n)
## update beta: refer to Wikipedia
newbetak = solve(t(X) %*% W %*% X) %*% t(X) %*%
(W %*% X %*% betak + as.matrix(Y) - as.numeric(expit(Xbeta)))
## difference between beta from current and previous iterations
beta_dist = norm(newbetak - betak, type="2")
betak = newbetak
cat("iteration",iter,"\n")
cat("distance between beta", beta_dist, "\n")
cat("betak ", betak[1], betak[2],betak[3],"\n")
## stopping criteria
if (iter > imax) {finish = T; conv = "diverge"}
else if (beta_dist < 0.00001) {finish = T; conv = "converge"}
}
betaF_logit_link = betak
cov_mat_betaF_logit_link = solve(t(X) %*% W %*% X)
sd_betaF_logit_link = sqrt(diag(cov_mat_betaF_logit_link))
table2 = cbind(betaF_logit_link,sd_betaF_logit_link)
colnames(table2) = c("Estimate", "Std. Error")
rownames(table2) = c("(intercept)", "spontaneous", "induced")
knitr::kable(table2, digits = 4)
summary(glm(case ~ spontaneous+induced, data = infert, family = binomial()))
## initial values
beta0_vec = c(-1,1,1)
X = as.matrix(cbind(one=rep(1,n),infert[,c(6,4)]))
Y = infert[5]
iter = 0                 ### track on the number of iterations
imax = 200               ### maximum iterations
betak = beta0_vec
conv = NULL              ### convergence status
finish = F               ### while loop status
weight = function(x){-x/sqrt(2*pi) * exp(-x^2/2)}
## iterations starts here
while (finish == F) {
iter = iter + 1
## calculate weight matrix (W)
Xbeta = X %*% betak
w = as.numeric(dnorm(Xbeta)/(pnorm(Xbeta)*(1-pnorm(Xbeta))))
W = diag(w, nrow=n, ncol=n)
newbetak = betak +
solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% (as.matrix(Y) - pnorm(Xbeta))
## difference between beta from current and previous iterations
beta_dist = norm(newbetak - betak, type="2")
betak = newbetak
cat("iteration",iter,"\n")
cat("distance between beta", beta_dist, "\n")
cat("betak ", betak[1], betak[2],betak[3],"\n")
## stopping criteria
if (iter > imax) {finish = T; conv = "diverge"}
else if (beta_dist < 0.00001) {finish = T; conv = "converge"}
}
betaF_probit_link = betak
cov_mat_betaF_probit_link = solve(t(X) %*% W %*% X)
sd_betaF_probit_link = sqrt(diag(cov_mat_betaF_probit_link))
table3 = cbind(betaF_probit_link,sd_betaF_probit_link)
colnames(table3) = c("Estimate", "Std. Error")
rownames(table3) = c("(intercept)", "spontaneous", "induced")
knitr::kable(table3, digits = 4)
summary(glm(case ~ spontaneous+induced, data = infert, family = binomial(link=probit)))
ratpups = read.csv("C:/Users/maoma/Desktop/2024 spring/BST513/Homework/ratpups.txt", sep="")
## create dummy variables
ratpups$low = ifelse(ratpups$dose==1,1,0)
ratpups$high = ifelse(ratpups$dose==2,1,0)
## create the variable: litter size
m = max(ratpups$litter)
N = dim(ratpups)[1]
litter_size = rep(0,m)
for (i in c(1:m)){litter_size[i] = sum(ratpups$litter == i)}
ratpups$litter_size = rep(litter_size,litter_size)
## reorder the columns with fixed effect first,
## followed by variables for random effect and outcome
ratpups = ratpups[,c("litter_size", "sex", "low", "high", "dose",
"num", "litter", "wgt")]
## created the matrix for algorithm
ones = rep(1, N)
Y = ratpups$wgt
X = ratpups[,c("litter_size", "sex", "low", "high")]
X = as.matrix(cbind(ones, X))
Z = matrix(0, nrow = N, ncol = m)
for (i in c(1:N)){
for (j in c(1:m)){
Z[i,j] = (ratpups$litter[i] == j)}}
## r function output for comparison
library(nlme)
model_formula = wgt ~ litter_size + sex + low + high
random_effects = ~ 1 | litter
fit_lme_ratpups = lme(fixed = model_formula,
random = random_effects,
data = ratpups, method = "ML")
fit_lme_ratpups
fit_lme_ratpups$coefficients$random
## step 1 initial values
beta0_vec = c(5,-1,1,-1,-1);            betak = beta0_vec
b0_vec = rep(.2,27);              bk = b0_vec
# b0_vec = unlist(fit_lme_ratpups$coefficients$random);  bk = b0_vec
# beta0_vec = unlist(fit_lme_ratpups$coefficients$fixed);  betak = beta0_vec
iter = 0                 ### track on the number of iterations
imax = 200               ### maximum iterations
conv = NULL              ### convergence status
finish = F               ### while loop status
while (finish == F) {
iter = iter + 1
## step 2 E step
epsilon = Y - X %*% betak - Z %*% bk
sigma_sq_e_k = as.numeric(t(t(Z) %*% epsilon) %*% (t(Z) %*% epsilon) /N)
sigma_sq_b_k = as.numeric(t(bk) %*% bk)/m
#Dk = bk %*% t(bk)/m
Dk = sigma_sq_b_k * diag(m)
Vk = sigma_sq_e_k * diag(N) + Z %*% Dk %*% t(Z)
## step 3 M step
newbetak = solve(t(X) %*% Vk %*% X) %*% t(X) %*% Vk %*% Y
newbk = Dk %*% t(Z) %*% solve(Vk) %*% (Y - X %*% newbetak)
## difference between beta from current and previous iterations
beta_dist = norm(newbetak - betak, type="2")
betak = newbetak
bk = newbk
cat("iteration",iter,"\n")
cat("distance between beta", beta_dist, "\n")
cat("betak ", betak[1], betak[2], betak[3], betak[4], betak[5],"\n")
## stopping criteria
if (iter > imax) {finish = T; conv = "diverge"}
else if (beta_dist < 0.00001) {finish = T; conv = "converge"}
}
bk
